{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Pipeline Preparation\n",
    "Follow the instructions below to help you create your ETL pipeline.\n",
    "### 1. Import libraries and load datasets.\n",
    "- Import Python libraries\n",
    "- Load `messages.csv` into a dataframe and inspect the first few lines.\n",
    "- Load `categories.csv` into a dataframe and inspect the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            message  \\\n",
       "0   2  Weather update - a cold front from Cuba that c...   \n",
       "1   7            Is the Hurricane over or is it not over   \n",
       "2   8                    Looking for someone but no name   \n",
       "3   9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4  12  says: west side of Haiti, rest of the country ...   \n",
       "\n",
       "                                            original   genre  \n",
       "0  Un front froid se retrouve sur Cuba ce matin. ...  direct  \n",
       "1                 Cyclone nan fini osinon li pa fini  direct  \n",
       "2  Patnm, di Maryani relem pou li banm nouvel li ...  direct  \n",
       "3  UN reports Leogane 80-90 destroyed. Only Hospi...  direct  \n",
       "4  facade ouest d Haiti et le reste du pays aujou...  direct  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load messages dataset\n",
    "messages = pd.read_csv('data\\disaster_messages.csv')\n",
    "# Remove duplicates\n",
    "messages = messages[~messages.duplicated(subset='id')]\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
       "       'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather', 'direct_report', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load categories dataset\n",
    "categories = pd.read_csv('data\\disaster_categories.csv')\n",
    "# Remove duplicates\n",
    "categories = categories[~categories.duplicated(subset='id')]\n",
    "\n",
    "# Split categories into 36 separate columns\n",
    "cat_split = categories['categories'].str.split(';', expand=True)\n",
    "# Rename the categories columns; use the first row to infer the names\n",
    "cat_split.columns = cat_split.iloc[0,:].apply(lambda x : x[:-2])\n",
    "# Convert the entries to 0/1\n",
    "cat_split = cat_split.applymap(lambda x : x[-1]).astype('int')\n",
    "# Replace erroneous values\n",
    "cat_split.replace({2:0}, inplace=True)\n",
    "# Bring back the 'id' column (will be joined on index)\n",
    "categories = cat_split.join(categories['id'])\n",
    "categories.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Merge datasets.\n",
    "- Merge the messages and categories datasets using the common id\n",
    "- Assign this combined dataset to `df`, which will be cleaned in the following steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26180, 4) (26180, 37) (26180, 40)\n"
     ]
    }
   ],
   "source": [
    "# merge datasets\n",
    "df = pd.merge(messages, categories, on='id', how='outer', validate='1:1')\n",
    "print(messages.shape, categories.shape, df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    15339\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a closer look at the labels\n",
    "\n",
    "# import seaborn as sn\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure(figsize=(13, 10))\n",
    "# sn.heatmap(categories.drop(columns='id').corr(), ax=fig.gca(), cmap='rocket_r')\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "df.query(\"aid_related == 0\")[['medical_help',\n",
    "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
    "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
    "       'missing_people', 'refugees', 'death', 'other_aid']].sum(axis=1).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove duplicates.\n",
    "- Check how many duplicates are in this dataset.\n",
    "- Drop the duplicates.\n",
    "- Confirm duplicates were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of duplicates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Save the clean dataset into an sqlite database.\n",
    "You can do this with pandas [`to_sql` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html) combined with the SQLAlchemy library. Remember to import SQLAlchemy's `create_engine` in the first cell of this notebook to use it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "df.to_sql('merged', engine, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Use this notebook to complete `etl_pipeline.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database based on new datasets specified by the user. Alternatively, you can complete `etl_pipeline.py` in the classroom on the `Project Workspace IDE` coming later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(messages_filepath, categories_filepath):\n",
    "    \"\"\"Load the datasets containing messages and categories.\n",
    "\n",
    "    Args:\n",
    "        messages_filepath: str\n",
    "            Path to a .csv file with the messages dataset.\n",
    "        categories_filepath: str\n",
    "            Path to a .csv file with the corresponding categories.\n",
    "\n",
    "    Returns:\n",
    "        A combined dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # load messages dataset\n",
    "    messages = pd.read_csv(messages_filepath)\n",
    "\n",
    "    # load categories dataset\n",
    "    categories = pd.read_csv(categories_filepath)\n",
    "\n",
    "    # Merge datasets\n",
    "    df = pd.merge(messages, categories, on='id', how='outer')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Cleans the megred dataframe of messages and categories.\"\"\"\n",
    "\n",
    "    # Remove duplicates\n",
    "    df = df[~df.duplicated(subset='id')]\n",
    "\n",
    "    # Split categories into 36 separate columns\n",
    "    cat_split = df['categories'].str.split(';', expand=True)\n",
    "    # Rename the categories columns; use the first row to infer the names\n",
    "    cat_split.columns = cat_split.iloc[0,:].apply(lambda x : x[:-2])\n",
    "    # Convert the entries to 0/1\n",
    "    cat_split = cat_split.applymap(lambda x : x[-1]).astype('int')\n",
    "    # Replace erroneous values ('2' corresponds to the 'No' answer)\n",
    "    cat_split.replace({2:0}, inplace=True)\n",
    "    # Combine with the rest of the dataframe\n",
    "    df = df.loc[:, df.columns != 'categories'].join(cat_split)\n",
    "    # Remove uninformative columns\n",
    "    df = df.loc[:, df.columns != 'child_alone']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_data(df, database_filename):\n",
    "    \"\"\"Save the cleaned dataset to a SQLite database.\n",
    "\n",
    "    Args:\n",
    "        df: pandas.DataFrame\n",
    "            Cleaned dataset; will be saved as a table called 'merged'.\n",
    "        database_filename: str\n",
    "            Filename of the database including the '.db' file extension.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove the file if it already exists\n",
    "    if os.path.exists(database_filename):\n",
    "        os.remove(database_filename)\n",
    "\n",
    "    engine = create_engine('sqlite:///{}'.format(database_filename))\n",
    "    df.to_sql('merged', engine, index=False)\n",
    "\n",
    "\n",
    "    \n",
    "df = load_data('data\\disaster_messages.csv', 'data\\disaster_categories.csv')\n",
    "\n",
    "df = clean_data(df)\n",
    "\n",
    "save_data(df, 'DisasterResponse.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
